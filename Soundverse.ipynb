{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e0da2b-18d9-4e63-8ec3-acf56ea75452",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "885e32cd-a8cf-47aa-8578-7ec2ada114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "de9581c2-a896-460f-80ce-e0fc87b40dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3370d3c4-ea5e-437a-badf-b582da121dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QKV(X, W_q, W_k, W_v):\n",
    "    Q = np.dot(X, W_q)\n",
    "    K = np.dot(X, W_k)\n",
    "    V = np.dot(X, W_v)\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "deaba38a-97ba-464f-a6ba-81f9d1f3f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(Q, K, V):\n",
    "    d= Q.shape[-1]\n",
    "    attention_scores = np.dot(Q, K.T) / np.sqrt(d)\n",
    "    attention_weights= softmax(attention_scores)\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2ece030b-06c1-4bbd-8229-9f2063638ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "Q, K, V = QKV(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2c8414d4-db6b-4cda-9852-a5e7f91c202f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6604769 2.6604769]\n",
      " [2.3395231 3.3395231]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9d843-dace-485d-abab-e4749c6b73da",
   "metadata": {},
   "source": [
    "## Multihead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1c27bc05-6a9c-4286-9d9a-bd831567ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_softmax(x):\n",
    "    one_hot = np.zeros_like(x)\n",
    "    one_hot[np.arange(len(x)), np.argmax(x, axis=-1)] = 1\n",
    "    return one_hot\n",
    "def QKV(X, W_q, W_k, W_v):\n",
    "    Q = np.dot(X, W_q)\n",
    "    K = np.dot(X, W_k)\n",
    "    V = np.dot(X, W_v)\n",
    "    return Q, K, V\n",
    "def self_attention(Q, K, V):\n",
    "    d= Q.shape[-1]\n",
    "    attention_scores = np.dot(Q, K.T) / np.sqrt(d)\n",
    "    attention_weights= hard_softmax(attention_scores)\n",
    "    output = np.dot(attention_weights, V)\n",
    "    return output\n",
    "    \n",
    "def multi_head_attention(Q, K, V, n):\n",
    "    d= Q.shape[-1]\n",
    "    assert d % n == 0, \"Embedding dimension must be divisible by no. of heads\"\n",
    "    head_dim = d // n\n",
    "    Q_heads = np.split(Q, n, axis=-1)\n",
    "    K_heads = np.split(K, n, axis=-1)\n",
    "    V_heads = np.split(V, n, axis=-1)\n",
    "    \n",
    "    heads = [self_attention(q, k, v) for q, k, v in zip(Q_heads, K_heads, V_heads)]\n",
    "    \n",
    "    multihead = np.concatenate(heads, axis=-1)\n",
    "    return multihead\n",
    "\n",
    "Q = np.array([[1, 0], [0, 1]])\n",
    "K = np.array([[1, 0], [0, 1]])\n",
    "V = np.array([[1, 0], [0, 1]])\n",
    "n = 2\n",
    "output = multi_head_attention(Q, K, V, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "88aaef3f-dd6a-4a89-8452-13a9b919feeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5c5d6-7813-4433-8098-c94c8878b741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
